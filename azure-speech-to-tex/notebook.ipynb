{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Speech To Text\n",
    "\n",
    "https://speech.microsoft.com/portal/269180473b2a452195896b8b908ae66d/speechtotexttool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/vincent/.local/share/virtualenvs/bert-summarize-HXPBLwpH/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load 'ml6team/mbart-large-cc25-cnn-dailymail-nl'. Make sure that:\n\n- 'ml6team/mbart-large-cc25-cnn-dailymail-nl' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'ml6team/mbart-large-cc25-cnn-dailymail-nl' is the correct path to a directory containing a 'config.json' file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/.local/share/virtualenvs/bert-summarize-HXPBLwpH/lib/python3.7/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, pretrained_config_archive_map, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output_hidden_states\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_attentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output_attentions\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorchscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchscript\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Only used by PyTorch models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-9134150cdfe2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m undisputed_best_model = transformers.modeling_bart.BartForConditionalGeneration.from_pretrained(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m\"ml6team/mbart-large-cc25-cnn-dailymail-nl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;32m~/.local/share/virtualenvs/bert-summarize-HXPBLwpH/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m           \u001b[0mderived\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0marchitecture\u001b[0m \u001b[0madding\u001b[0m \u001b[0mmodules\u001b[0m \u001b[0mon\u001b[0m \u001b[0mtop\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbase\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mis_parallelizable\u001b[0m\u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mA\u001b[0m \u001b[0mflag\u001b[0m \u001b[0mindicating\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0msupports\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mparallelization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m         - **main_input_name** (`str`) -- The name of the principal input to the model (often `input_ids` for\n\u001b[0m\u001b[1;32m    439\u001b[0m           NLP models, `pixel_values` for vision models and `input_values` for speech models).\n\u001b[1;32m    440\u001b[0m     \"\"\"\n",
      "\u001b[0;32m~/.local/share/virtualenvs/bert-summarize-HXPBLwpH/lib/python3.7/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mtokenizer_class\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moptional\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             The name of the associated tokenizer class to use (if none is set, will use the tokenizer associated to the\n\u001b[0;32m--> 200\u001b[0;31m             model by default).\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mprefix\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moptional\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0madded\u001b[0m \u001b[0mat\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbeginning\u001b[0m \u001b[0mof\u001b[0m \u001b[0meach\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0mcalling\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/bert-summarize-HXPBLwpH/lib/python3.7/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, pretrained_config_archive_map, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"min_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do_sample\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"early_stopping\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_beams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"num_beams\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load 'ml6team/mbart-large-cc25-cnn-dailymail-nl'. Make sure that:\n\n- 'ml6team/mbart-large-cc25-cnn-dailymail-nl' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'ml6team/mbart-large-cc25-cnn-dailymail-nl' is the correct path to a directory containing a 'config.json' file\n\n"
     ]
    }
   ],
   "source": [
    "undisputed_best_model = transformers.modeling_bart.BartForConditionalGeneration.from_pretrained(\n",
    "    \"ml6team/mbart-large-cc25-cnn-dailymail-nl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'transformers' has no attribute 'MBartTokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f01701ccffca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#     \"ml6team/mbart-large-cc25-cnn-dailymail-nl-finetune\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMBartTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"facebook/mbart-large-cc25\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m summarization_pipeline = transformers.pipeline(\n\u001b[1;32m     12\u001b[0m     \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"summarization\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'transformers' has no attribute 'MBartTokenizer'"
     ]
    }
   ],
   "source": [
    "# model_uri = \"ml6team/mbart-large-cc25-cnn-dailymail-nl-finetune\"\n",
    "# classifier = pipeline(task=\"summarization\", model=model_uri)\n",
    "\n",
    "# model = BartForConditionalGeneration.from_pretrained(model_uri)\n",
    "# tokenizer = BartTokenizer.from_pretrained(model_uri)\n",
    "\n",
    "# undisputed_best_model = transformers.MBartForConditionalGeneration.from_pretrained(\n",
    "#     \"ml6team/mbart-large-cc25-cnn-dailymail-nl-finetune\"\n",
    "# )\n",
    "tokenizer = transformers.MBartTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "summarization_pipeline = transformers.pipeline(\n",
    "    task=\"summarization\",\n",
    "    model=undisputed_best_model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "summarization_pipeline.model.config.decoder_start_token_id = tokenizer.lang_code_to_id[\n",
    "    \"nl_XX\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = \"Welkom bouwkundig ingenieurs van de universiteit Gent en beton is jouw onderzoeksgebied het onderzoeksgebied? Ja, slechte pers, hè? Inderdaad, Het is voor materialen overal hanteren komen en moeten we toch weer herleven Natuurlijk, ja, maar die en dan heb ik het nog niet gehad. Over bruggen die instorten en Brusselse tunnels waarvan stukje uit het plafond. Valt is dat imago van beton? Ja, Maar dat is alsof je tegen muziek zou moeten zijn Omdat er slechte muziek bestaat. Natuurlijk bestaat er slecht beton. Ga jij mijn interview nu te saboteren? Wil je voor beton zijn? Wat wat de standaard nu schrijft vandaag op zijn voorpagina dat dat we Omdat we met teveel beton zitten in brand staat dan voor te veel te veel bouwactiviteiten in Vlaanderen. Dat is toch ontegensprekelijk? Lijkt mij. Wordt de Vlamingen vooral een baksteen in hun maag gehad, altijd beton, haar gebruiken en het feit dat we nu inderdaad heel veel betonnen hebben. Die heeft er wel voor gezorgd dat we een mooie kwaliteit, daar hebben we al maar moeten Natuurlijk wel beton naar beneden gaan krijgen, Maar dat gaat niet met de betonstop gebeuren, want dan gaan we altijd moeten blijven bouwen. Ja en Dat is Dat is politiek. Dat zijn de politieke beslissingen waarin hoeveel er gebouwd, toch? Daar bent u niet mee bezig. U bent bezig met materiaal op zich om het beter te maken, ja. Zullen, we zullen eerst eens beginnen met te vertellen wat beton juist is, wat ze daar wat zit er allemaal in. Je begint eigenlijk met Samantha water, dan zei ik hetgeen wat dat alles Samen gaat binden. We voeren nou ook nog wel zand van keien aan toe en Als je alles goed gaan mengen, dan krijg je een grijzer vele massa eigenlijk. Ik had in een bepaalde vorm en dan krijg je het Omdat we overal zien. Ik weet, Ik weet wat keien zijn, Ik weet wat santiz Ik weet dat water is. Ik weet ergens iemand kan kopen in een zakje, maar eigenlijk weet ik niet, want Dat is cement, dus We moeten beginnen met bestanddelen, dus Ik ben heel blij met klei houden de bestanddelen. En, We gaan op komen halen handel plan op korte termijn komt uit een fabriek ja, komt uit. De voorraad is geen delle stoffen. Nee, ja. Wat zet je? In het boek is de sleutel plantages werking hoe komt het dat de centrale en Als we alles kan opwarmen tot 1450 °c, dan krijgen we die grijze massa, kunnen we verder haver malen kan je dan in Je moet u moet kelk opwarmen tot 1450 graden en kalk dat Dat is zoals in krijt en en gemoederen, dat wordt het grijs ja. De andere kristalletjes, gevormd door een. Dat is makkelijk, hè? En en dan doe je daar water bij, dan verhardt het ja door door uitdroging of door dus al die materialen die grijze massa eigenlijk Als we daar water in gaan toe naar bepaalde bestanddelen aan die in oplossing komen. En dan gaan die eigenlijk historisch gaan vormen die dan op het cement gaan iets gaan vormen en uiteindelijk groeit alles toe en op die manier krijg je iets vast. En Als we nog wel langer wachten, dan kunnen die kristalletjes meer naar voren krijgen wij een sterk materiaal, dus op die manier werd cement ok. Zo werkt ze mint maar om beton te krijgen, doe je daar dan nog? Rijk zal stof keitjes bij staan, nu dan wel bijvoorbeeld enkel maar cement en. We hadden er zo herbruiken, dan krijg je wel een grijze massa opnieuw. Maar had hij veel te duur zijn, want zo iemand is het duurste bestand helemaal doen, dus vandaar voeren we daar zand en keien aan toe. Iets goedkoper om dan eigenlijk, maar had het de grijze massa te krijgen en toch nog altijd een sterk materiaal maar om te om te missen om bakstenen aan elkaar te mixen heb je morgen nodig. Dan is dan cement, water en zand bijvoorbeeld. Hoe wordt dat dan gaan maken en waar dienen dan die keitjes precies voor die die dan nog toevoegt om beton te zeggen? Om dan eigenlijk ook een mooie kool verdeling te krijgen om eigenlijk nog meer volume te krijgen. Van het al gaat dan, want In de Mortel haar meer volume cementen zitten. Maar wij willen eigenlijk het volume cement naar beneden krijgen omwille van die prijs ook weer omwille van de kunst zijn, ook van voor het ecologische. Dat doe je instellingen eigenlijk ecologische doelstellingen probleem met cement is eigenlijk, Dat is een beetje die in mijn winkel dan om te zeggen, doe geen, nu ga voorlezen, Maar we werken eraan. Maar Als we cement gaan maken, dus We moeten dat gaan opruimen. Als we iets gaan opwarmen, kalkhoudend dan calciumoxide s. Hebben we dan eigenlijk co twee die uitgestoten wordt? Er was wel een heel erg enorme productie, zoals beton. Daarom hebben die iets lager is instrument of andere stoffen aan toevoegen. Kunnen we ook al die co twee uitstoot willen verminderen? Dus Er zijn verschillende redenen om de hoeveelheid cement in dat mengsel dat we dan betonnen. Maar Omdat Omdat de hoeveelheid zijn zo klein mogelijk te houden. Dat is goedkoper en als ecologisch Natuurlijk wel een minimumhoeveelheid hebben in die sector te begrijpen. Sven herinner jij je ook in vaag weg dat wij Misschien alsnog een jongens en wetenschap van die dingen, ja jongens in wetenschap. Iets gedaan hebben over over beton en dat de Romeinen ja, jij helemaal beton. Ja dat hij dat al laat en dat we dat vergeten zijn en dat dat door iemand is heruitgevonden. Cloak, wilt u dat nu nog eens opnieuw zeggen? Ja, kan u dat bevestigen hè? Als discriminatie, bijvoorbeeld door mannen In het stadshuus wereld is nabij Napels wat je had een visie virus en de visie is heel veel Assen uitgestuurd en die Assen, die hadden ongeveer dezelfde eigenschappen Als het cement dat we nu hebben. Er werd verwarmd In de vulkanen, kwam In het werd heel stil of andere manier hadden ze dan het grijze poeder, Dat was verbrande kalk, maar wisten zij veel ja enzovoort nou opeens wel water aan toe en dat weggevaagd. Ja vandaar dat ze eigenlijk het begon hebben uitgevonden, komt van het Latijnse concreet dus. En Dat is eigenlijk mijn twee muren gebouwd, waarbij daar dus om dan die grijze massa gestort is. En dan is dan het sterke beton. En op die manier hebben ze dan hun aquaducten gebouwd om Pantheon en zo verder enzovoort, want dan zijn we daar inderdaad vergeten de Romein die konden dat wel gebruikt, maar In de donkere tijden In de Middeleeuwen waren dan volledig vergeten en dan kwamen opeens terecht nabij de Nederlandse Portland, waar wij dat er eigenlijk ook? Een bepaalde stof was en waarbij dat boek met water van hydraulische was ontvoering reageren met water materiaal krijgen dat dan op nieuw prijs was en daar werd dan het portlandcement genoemd. Omdat er dan leek op rij coach rotsen die van de eilandje waren en dat werd aangeplant. Die patenteert excuseer en sindsdien wordt er globaal in overal waar dat maar kan hebben en rond welke tijd zitten we dan ruim achttienhonderd? Ik ben in achttienhonderd als beton u maakt verbeterd beton of u studeert op hoe beter het beton kunnen maken. Wat is er mis met klassiek beton? Het scheurt dus. Beton is eigenlijk een materiaal dat heel sterke. Ze kunt daar heel wereld masker opzetten, kunt u hoogste toeren ze daarmee bouwen. Toen ons Natuurlijk ook Als het heel veel druk op, dus dat kan heel erg goed, doe ik ook niet, maar stel niet bijvoorbeeld een ander materiaal zou nemen, een elastiekje bijvoorbeeld en zouter aan haar trekken had dat ook. Kapot gaan zelfs Natuurlijk met beton. Als we daar gaan aantrekken, dan gaat het ook als vuur op zich is dat geen probleem, maar beton komt altijd met wapening, ook om die trein land op te nemen. Als we die schuur hebben en waarvoor je het water en agressieve stoffen train naar binnen, dan kunnen die wapeningsstaven gaan corroderen.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_no_trunc = tokenizer(long_text, max_length=None, return_tensors='pt', truncation=False)\n",
    "\n",
    "# get batches of tokens corresponding to the exact model_max_length\n",
    "chunk_start = 0\n",
    "chunk_end = tokenizer.model_max_length  # == 1024 for Bart\n",
    "inputs_batch_lst = []\n",
    "while chunk_start <= len(inputs_no_trunc['input_ids'][0]):\n",
    "    inputs_batch = inputs_no_trunc['input_ids'][0][chunk_start:chunk_end]  # get batch of n tokens\n",
    "    inputs_batch = torch.unsqueeze(inputs_batch, 0)\n",
    "    inputs_batch_lst.append(inputs_batch)\n",
    "    chunk_start += tokenizer.model_max_length  # == 1024 for Bart\n",
    "    chunk_end += tokenizer.model_max_length  # == 1024 for Bart\n",
    "\n",
    "# generate a summary on each batch\n",
    "summary_ids_lst = [model.generate(inputs, num_beams=4, max_length=100, early_stopping=True) for inputs in inputs_batch_lst]\n",
    "\n",
    "# decode the output and join into one string with one paragraph per summary batch\n",
    "summary_batch_lst = []\n",
    "for summary_id in summary_ids_lst:\n",
    "    summary_batch = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_id]\n",
    "    summary_batch_lst.append(summary_batch[0])\n",
    "summary_all = '\\n'.join(summary_batch_lst)\n",
    "\n",
    "print(summary_all)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
